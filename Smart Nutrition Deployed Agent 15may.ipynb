{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# AI Service Deployment Notebook\n",
    "This notebook contains steps and code to test, promote, and deploy an Agent as an AI Service.\n",
    "\n",
    "**Note:** Notebook code generated using Agent Lab will execute successfully.\n",
    "If code is modified or reordered, there is no guarantee it will successfully execute.\n",
    "For details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n",
    "\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "## Contents\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1. Setup\n",
    "2. Initialize all the variables needed by the AI Service\n",
    "3. Define the AI service function\n",
    "4. Deploy an AI Service\n",
    "5. Test the deployed AI Service\n",
    "\n",
    "## 1. Set up the environment\n",
    "\n",
    "Before you can run this notebook, you must perform the following setup tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to WML\n",
    "This cell defines the credentials required to work with watsonx API for both the execution in the project, \n",
    "as well as the deployment and runtime execution of the function.\n",
    "\n",
    "**Action:** Provide the IBM Cloud personal API key. For details, see\n",
    "<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_ibm\n",
    "!pip install ibm_watsonx_ai\n",
    "!pip install langgraph\n",
    "!pip install langchain_community\n",
    "!pip install langchain_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "import getpass\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://eu-de.ml.cloud.ibm.com\",\n",
    "    api_key=getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to a space\n",
    "A space will be be used to host the promoted AI Service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_id = \"6f5c22b8-3bd7-4e20-a775-12c135617056\"\n",
    "client.set.default_space(space_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote asset(s) to space\n",
    "We will now promote assets we will need to stage in the space so that we can access their data from the AI service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_project_id = \"bcc191f5-756f-4d90-bae1-78bcf20733cc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the AI service function\n",
    "We first need to define the AI service function\n",
    "\n",
    "### 2.1 Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"space_id\": space_id,\n",
    "}\n",
    "\n",
    "def gen_ai_service(context, params = params, **custom):\n",
    "    # import dependencies\n",
    "    from langchain_ibm import ChatWatsonx\n",
    "    from ibm_watsonx_ai import APIClient\n",
    "    from langchain_core.messages import AIMessage, HumanMessage\n",
    "    from langchain.tools import WikipediaQueryRun\n",
    "    from langchain_community.utilities import WikipediaAPIWrapper\n",
    "    from langchain_community.tools import DuckDuckGoSearchRun\n",
    "    from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    import json\n",
    "\n",
    "    model = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "    \n",
    "    service_url = \"https://eu-de.ml.cloud.ibm.com\"\n",
    "    # Get credentials token\n",
    "    credentials = {\n",
    "        \"url\": service_url,\n",
    "        \"token\": context.generate_token()\n",
    "    }\n",
    "\n",
    "    # Setup client\n",
    "    client = APIClient(credentials)\n",
    "    space_id = params.get(\"space_id\")\n",
    "    client.set.default_space(space_id)\n",
    "\n",
    "\n",
    "    def create_chat_model(watsonx_client):\n",
    "        parameters = {\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1\n",
    "        }\n",
    "\n",
    "        chat_model = ChatWatsonx(\n",
    "            model_id=model,\n",
    "            url=service_url,\n",
    "            space_id=space_id,\n",
    "            params=parameters,\n",
    "            watsonx_client=watsonx_client,\n",
    "        )\n",
    "        return chat_model\n",
    "    \n",
    "    def get_schema_model(original_json_schema):\n",
    "        # Create a pydantic base model class from the tool's JSON schema\n",
    "        from datamodel_code_generator import DataModelType, PythonVersion\n",
    "        from datamodel_code_generator.model import get_data_model_types\n",
    "        from datamodel_code_generator.parser.jsonschema import JsonSchemaParser\n",
    "        from typing import Optional\n",
    "        from pydantic import BaseModel, Field, constr\n",
    "        import json\n",
    "    \n",
    "        json_schema = json.dumps(original_json_schema)\n",
    "    \n",
    "        data_model_types = get_data_model_types(\n",
    "            DataModelType.PydanticV2BaseModel,\n",
    "            target_python_version=PythonVersion.PY_311\n",
    "        )\n",
    "    \n",
    "        # Returns the python class code as a string\n",
    "        parser = JsonSchemaParser(\n",
    "            json_schema,\n",
    "            data_model_type=data_model_types.data_model,\n",
    "            data_model_root_type=data_model_types.root_model,\n",
    "            data_model_field_type=data_model_types.field_model,\n",
    "            data_type_manager_type=data_model_types.data_type_manager,\n",
    "            dump_resolve_reference_action=data_model_types.dump_resolve_reference_action,\n",
    "        )\n",
    "    \n",
    "        model_code = parser.parse()\n",
    "    \n",
    "        full_code = model_code\n",
    "        namespace = {\n",
    "            \"Field\": Field,\n",
    "            \"constr\": constr,\n",
    "            \"Optional\": Optional\n",
    "        }\n",
    "        value = exec(full_code, namespace)\n",
    "        value = exec(\"Model.model_rebuild()\", namespace)\n",
    "        pydantic_model = namespace['Model']\n",
    "        return pydantic_model\n",
    "    \n",
    "    def get_remote_tool_descriptions():\n",
    "        remote_tool_descriptions = {}\n",
    "        remote_tool_schemas = {}\n",
    "        import requests\n",
    "    \n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f'Bearer {context.generate_token()}'\n",
    "        }\n",
    "    \n",
    "        tool_url = \"https://private.api.eu-de.dataplatform.cloud.ibm.com\"\n",
    "        \n",
    "        remote_tools_response = requests.get(f'{tool_url}/wx/v1-beta/utility_agent_tools', headers = headers)\n",
    "        remote_tools = remote_tools_response.json()\n",
    "        \n",
    "        for resource in remote_tools[\"resources\"]:\n",
    "            tool_name = resource[\"name\"]\n",
    "            tool_description = resource[\"description\"]\n",
    "            tool_schema = resource.get(\"input_schema\")\n",
    "            remote_tool_descriptions[tool_name] = tool_description\n",
    "            if (tool_schema):\n",
    "                remote_tool_schemas[tool_name] = get_schema_model(tool_schema)\n",
    "    \n",
    "        return remote_tool_descriptions, remote_tool_schemas\n",
    "        \n",
    "    tool_descriptions, tool_schemas = get_remote_tool_descriptions()\n",
    "    \n",
    "    def create_remote_tool(tool_name, context):\n",
    "        from langchain_core.tools import StructuredTool\n",
    "        from langchain_core.tools import Tool\n",
    "        import requests\n",
    "    \n",
    "        def call_tool( tool_input ):\n",
    "            body = {\n",
    "                \"tool_name\": tool_name,\n",
    "                \"input\": tool_input\n",
    "            }\n",
    "    \n",
    "            headers  = {\n",
    "                \"Accept\": \"application/json\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f'Bearer {context.get_token()}'\n",
    "            }\n",
    "    \n",
    "            tool_url = \"https://private.api.eu-de.dataplatform.cloud.ibm.com\"\n",
    "    \n",
    "            tool_response = requests.post(f'{tool_url}/wx/v1-beta/utility_agent_tools/run', headers = headers, json = body)\n",
    "    \n",
    "            if (tool_response.status_code > 400):\n",
    "                raise Exception(f'Error calling remote tool: {tool_response.json()}' )\n",
    "    \n",
    "            tool_output = tool_response.json()\n",
    "            return tool_response.json().get(\"output\")\n",
    "    \n",
    "        def call_tool_structured(**tool_input):\n",
    "            return call_tool(tool_input)\n",
    "    \n",
    "        def call_tool_unstructured(tool_input):\n",
    "            return call_tool(tool_input)\n",
    "        \n",
    "        remote_tool_schema = tool_schemas.get(tool_name)\n",
    "    \n",
    "        if (remote_tool_schema):\n",
    "            tool = StructuredTool(\n",
    "                name=tool_name,\n",
    "                description = tool_descriptions[tool_name],\n",
    "                func=call_tool_structured,\n",
    "                args_schema=remote_tool_schema\n",
    "            )\n",
    "            return tool    \n",
    "    \n",
    "    \n",
    "        \n",
    "        tool = Tool(\n",
    "            name=tool_name,\n",
    "            description = tool_descriptions[tool_name],\n",
    "            func=call_tool_unstructured\n",
    "        )\n",
    "        return tool            \n",
    "    \n",
    "    \n",
    "    def create_custom_tool(tool_name, tool_description, tool_code, tool_schema):\n",
    "        from langchain_core.tools import StructuredTool\n",
    "        import ast\n",
    "    \n",
    "        def call_tool(**kwargs):\n",
    "            tree = ast.parse(tool_code, mode=\"exec\")\n",
    "            function_name = tree.body[0].name\n",
    "            compiled_code = compile(tree, 'custom_tool', 'exec')\n",
    "            namespace = {}\n",
    "            exec(compiled_code, namespace)\n",
    "            return namespace[function_name](**kwargs)\n",
    "            \n",
    "        tool = StructuredTool(\n",
    "            name=tool_name,\n",
    "            description = tool_description,\n",
    "            func=call_tool,\n",
    "            args_schema=get_schema_model(tool_schema)\n",
    "        )\n",
    "        return tool\n",
    "    \n",
    "    def create_custom_tools():\n",
    "        custom_tools = []\n",
    "    \n",
    "\n",
    "    def create_tools(inner_client, context):\n",
    "        tools = []\n",
    "        top_k_results = 5\n",
    "        wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=top_k_results))\n",
    "        tools.append(wikipedia)\n",
    "        max_results = 10\n",
    "        search = DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(max_results=max_results))\n",
    "        tools.append(search)\n",
    "        \n",
    "        tools.append(create_remote_tool(\"GoogleSearch\", context))\n",
    "        tools.append(create_remote_tool(\"Weather\", context))\n",
    "        tools.append(create_remote_tool(\"WebCrawler\", context))\n",
    "        return tools\n",
    "    \n",
    "    def create_agent(model, tools, messages):\n",
    "        memory = MemorySaver()\n",
    "        instructions = \"\"\"\n",
    "# Notes\n",
    "- Use markdown syntax for formatting code snippets, links, JSON, tables, images, files.\n",
    "- Any HTML tags must be wrapped in block quotes, for example ```<html>```.\n",
    "- When returning code blocks, specify language.\n",
    "- Sometimes, things don't go as planned. Tools may not provide useful information on the first few tries. You should always try a few different approaches before declaring the problem unsolvable.\n",
    "- When the tool doesn't give you what you were asking for, you must either use another tool or a different tool input.\n",
    "- When using search engines, you try different formulations of the query, possibly even in a different language.\n",
    "- You cannot do complex calculations, computations, or data manipulations without using tools.\n",
    "- If you need to call a tool to compute something, always call it instead of saying you will call it.\n",
    "\n",
    "If a tool returns an IMAGE in the result, you must include it in your answer as Markdown.\n",
    "\n",
    "Example:\n",
    "\n",
    "Tool result: IMAGE(https://api.eu-de.dataplatform.cloud.ibm.com/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
    "Markdown to return to user: ![Generated image](https://api.eu-de.dataplatform.cloud.ibm.com/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
    "\n",
    "You are a helpful assistant that uses tools to answer questions in detail.\n",
    "When greeted, say Hi, I am watsonx.ai agent. How can I help you?\n",
    "You are a highly knowledgeable and helpful AI nutritionist.\n",
    "Provide \\\"personalized nutrition guidance based on my specific needs and goals.\n",
    "Understand user inputs via text, voice, or image like food photos, grocery labels.\n",
    "Please ask me clarifying questions to understand my dietary preferences, health conditions, and fitness goals.\n",
    "After understanding my requirements, generate a detailed and tailored nutrition plan, including meal suggestions, recipes, and tips for healthy eating.\n",
    "\"\"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                instructions += message[\"content\"]\n",
    "        graph = create_react_agent(model, tools=tools, checkpointer=memory, state_modifier=instructions)\n",
    "        return graph\n",
    "    \n",
    "    def convert_messages(messages):\n",
    "        converted_messages = []\n",
    "        for message in messages:\n",
    "            if (message[\"role\"] == \"user\"):\n",
    "                converted_messages.append(HumanMessage(content=message[\"content\"]))\n",
    "            elif (message[\"role\"] == \"assistant\"):\n",
    "                converted_messages.append(AIMessage(content=message[\"content\"]))\n",
    "        return converted_messages\n",
    "\n",
    "    def generate(context):\n",
    "        payload = context.get_json()\n",
    "        messages = payload.get(\"messages\")\n",
    "        inner_credentials = {\n",
    "            \"url\": service_url,\n",
    "            \"token\": context.get_token()\n",
    "        }\n",
    "\n",
    "        inner_client = APIClient(inner_credentials)\n",
    "        model = create_chat_model(inner_client)\n",
    "        tools = create_tools(inner_client, context)\n",
    "        agent = create_agent(model, tools, messages)\n",
    "        \n",
    "        generated_response = agent.invoke(\n",
    "            { \"messages\": convert_messages(messages) },\n",
    "            { \"configurable\": { \"thread_id\": \"42\" } }\n",
    "        )\n",
    "\n",
    "        last_message = generated_response[\"messages\"][-1]\n",
    "        generated_response = last_message.content\n",
    "\n",
    "        execute_response = {\n",
    "            \"headers\": {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"choices\": [{\n",
    "                    \"index\": 0,\n",
    "                    \"message\": {\n",
    "                       \"role\": \"assistant\",\n",
    "                       \"content\": generated_response\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return execute_response\n",
    "\n",
    "    def generate_stream(context):\n",
    "        print(\"Generate stream\", flush=True)\n",
    "        payload = context.get_json()\n",
    "        headers = context.get_headers()\n",
    "        is_assistant = headers.get(\"X-Ai-Interface\") == \"assistant\"\n",
    "        messages = payload.get(\"messages\")\n",
    "        inner_credentials = {\n",
    "            \"url\": service_url,\n",
    "            \"token\": context.get_token()\n",
    "        }\n",
    "        inner_client = APIClient(inner_credentials)\n",
    "        model = create_chat_model(inner_client)\n",
    "        tools = create_tools(inner_client, context)\n",
    "        agent = create_agent(model, tools, messages)\n",
    "\n",
    "        response_stream = agent.stream(\n",
    "            { \"messages\": messages },\n",
    "            { \"configurable\": { \"thread_id\": \"42\" } },\n",
    "            stream_mode=[\"updates\", \"messages\"]\n",
    "        )\n",
    "\n",
    "        for chunk in response_stream:\n",
    "            chunk_type = chunk[0]\n",
    "            finish_reason = \"\"\n",
    "            usage = None\n",
    "            if (chunk_type == \"messages\"):\n",
    "                message_object = chunk[1][0]\n",
    "                if (message_object.type == \"AIMessageChunk\" and message_object.content != \"\"):\n",
    "                    message = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": message_object.content\n",
    "                    }\n",
    "                else:\n",
    "                    continue\n",
    "            elif (chunk_type == \"updates\"):\n",
    "                update = chunk[1]\n",
    "                if (\"agent\" in update):\n",
    "                    agent = update[\"agent\"]\n",
    "                    agent_result = agent[\"messages\"][0]\n",
    "                    if (agent_result.additional_kwargs):\n",
    "                        kwargs = agent[\"messages\"][0].additional_kwargs\n",
    "                        tool_call = kwargs[\"tool_calls\"][0]\n",
    "                        if (is_assistant):\n",
    "                            message = {\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"step_details\": {\n",
    "                                    \"type\": \"tool_calls\",\n",
    "                                    \"tool_calls\": [\n",
    "                                        {\n",
    "                                            \"id\": tool_call[\"id\"],\n",
    "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
    "                                            \"args\": tool_call[\"function\"][\"arguments\"]\n",
    "                                        }\n",
    "                                    ] \n",
    "                                }\n",
    "                            }\n",
    "                        else:\n",
    "                            message = {\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"tool_calls\": [\n",
    "                                    {\n",
    "                                        \"id\": tool_call[\"id\"],\n",
    "                                        \"type\": \"function\",\n",
    "                                        \"function\": {\n",
    "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
    "                                            \"arguments\": tool_call[\"function\"][\"arguments\"]\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                    elif (agent_result.response_metadata):\n",
    "                        # Final update\n",
    "                        message = {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": agent_result.content\n",
    "                        }\n",
    "                        finish_reason = agent_result.response_metadata[\"finish_reason\"]\n",
    "                        if (finish_reason): \n",
    "                            message[\"content\"] = \"\"\n",
    "\n",
    "                        usage = {\n",
    "                            \"completion_tokens\": agent_result.usage_metadata[\"output_tokens\"],\n",
    "                            \"prompt_tokens\": agent_result.usage_metadata[\"input_tokens\"],\n",
    "                            \"total_tokens\": agent_result.usage_metadata[\"total_tokens\"]\n",
    "                        }\n",
    "                elif (\"tools\" in update):\n",
    "                    tools = update[\"tools\"]\n",
    "                    tool_result = tools[\"messages\"][0]\n",
    "                    if (is_assistant):\n",
    "                        message = {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"step_details\": {\n",
    "                                \"type\": \"tool_response\",\n",
    "                                \"id\": tool_result.id,\n",
    "                                \"tool_call_id\": tool_result.tool_call_id,\n",
    "                                \"name\": tool_result.name,\n",
    "                                \"content\": tool_result.content\n",
    "                            }\n",
    "                        }\n",
    "                    else:\n",
    "                        message = {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"id\": tool_result.id,\n",
    "                            \"tool_call_id\": tool_result.tool_call_id,\n",
    "                            \"name\": tool_result.name,\n",
    "                            \"content\": tool_result.content\n",
    "                        }\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            chunk_response = {\n",
    "                \"choices\": [{\n",
    "                    \"index\": 0,\n",
    "                    \"delta\": message\n",
    "                }]\n",
    "            }\n",
    "            if (finish_reason):\n",
    "                chunk_response[\"choices\"][0][\"finish_reason\"] = finish_reason\n",
    "            if (usage):\n",
    "                chunk_response[\"usage\"] = usage\n",
    "            yield chunk_response\n",
    "\n",
    "    return generate, generate_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AI Service function locally\n",
    "from ibm_watsonx_ai.deployments import RuntimeContext\n",
    "\n",
    "context = RuntimeContext(api_client=client)\n",
    "\n",
    "streaming = False\n",
    "findex = 1 if streaming else 0\n",
    "local_function = gen_ai_service(context,  space_id=space_id)[findex]\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_question = \"Change this question to test your function\"\n",
    "\n",
    "messages.append({ \"role\" : \"user\", \"content\": local_question })\n",
    "\n",
    "context = RuntimeContext(api_client=client, request_payload_json={\"messages\": messages})\n",
    "\n",
    "response = local_function(context)\n",
    "\n",
    "result = ''\n",
    "\n",
    "if (streaming):\n",
    "    for chunk in response:\n",
    "        print(chunk, end=\"\\n\\n\", flush=True)\n",
    "else:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Store and deploy the AI Service\n",
    "Before you can deploy the AI Service, you must store the AI service in your watsonx.ai repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up software specification for the AI service\n",
    "software_spec_id_in_project = \"261f3a33-cce4-4dda-9395-f7daa24b6a2d\"\n",
    "software_spec_id = \"\"\n",
    "\n",
    "try:\n",
    "\tsoftware_spec_id = client.software_specifications.get_id_by_name(\"ai-service-v6-a-software-specification\")\n",
    "except:\n",
    "    software_spec_id = client.spaces.promote(software_spec_id_in_project, source_project_id, space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the request and response schemas for the AI service\n",
    "request_schema = {\n",
    "    \"application/json\": {\n",
    "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"messages\": {\n",
    "                \"title\": \"The messages for this chat session.\",\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"role\": {\n",
    "                            \"title\": \"The role of the message author.\",\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"user\",\"assistant\"]\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"title\": \"The contents of the message.\",\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"role\",\"content\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"messages\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response_schema = {\n",
    "    \"application/json\": {\n",
    "        \"oneOf\": [{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service_stream\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices.\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"title\":\"The index of this result.\"},\"delta\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"content\":{\"description\":\"The contents of the message.\",\"type\":\"string\"},\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]},{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"description\":\"The index of this result.\"},\"message\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"},\"content\":{\"title\":\"Message content.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the AI service in the repository\n",
    "ai_service_metadata = {\n",
    "    client.repository.AIServiceMetaNames.NAME: \"Smart Nutrition Deployed Agent\",\n",
    "    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n",
    "    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n",
    "    client.repository.AIServiceMetaNames.CUSTOM: {},\n",
    "    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n",
    "    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema,\n",
    "    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n",
    "}\n",
    "\n",
    "ai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AI Service ID\n",
    "\n",
    "ai_service_id = client.repository.get_ai_service_id(ai_service_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the stored AI Service\n",
    "deployment_custom = {\n",
    "    \"avatar_icon\": \"Bot\",\n",
    "    \"avatar_color\": \"background\",\n",
    "    \"placeholder_image\": \"placeholder2.png\"\n",
    "}\n",
    "deployment_metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Smart Nutrition Deployed Agent\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.CUSTOM: deployment_custom,\n",
    "    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Change this description to reflect your particular agent\",\n",
    "    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n",
    "}\n",
    "\n",
    "function_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test AI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID of the AI Service deployment just created\n",
    "\n",
    "deployment_id = client.deployments.get_id(function_deployment_details)\n",
    "print(deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "remote_question = \"Change this question to test your function\"\n",
    "messages.append({ \"role\" : \"user\", \"content\": remote_question })\n",
    "payload = { \"messages\": messages }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.deployments.run_ai_service(deployment_id, payload)\n",
    "if \"error\" in result:\n",
    "    print(result[\"error\"])\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG \n",
    "!pip install openai faiss-cpu pandas tiktoken langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG code\n",
    "import pandas as pd\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG code \n",
    "# Load CSV directly\n",
    "df = pd.read_csv('/content/test.csv')\n",
    "print(\"CSV Loaded Successfully. Preview:\")\n",
    "display(df.head())\n",
    "\n",
    "# Function to create RAG chain from DataFrame\n",
    "def process_and_create_rag_from_df(df):\n",
    "    # Convert DataFrame to text\n",
    "    text_data = df.to_csv(index=False)\n",
    "\n",
    "    # Split into chunks using a token-aware splitter\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import tiktoken\n",
    "\n",
    "    # Use a RecursiveCharacterTextSplitter with tiktoken tokenizer\n",
    "    # Set chunk_size well below the OpenAI token limit per request\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Reduced chunk size significantly\n",
    "        chunk_overlap=200, # Adjust overlap as needed\n",
    "        length_function=lambda text: len(tiktoken.encoding_for_model(\"text-embedding-ada-002\").encode(text)), # Use token count as length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Common separators\n",
    "    )\n",
    "    texts = splitter.split_text(text_data)\n",
    "\n",
    "    # Create vector store\n",
    "    # Ensure your OpenAI API key is correctly set as an environment variable or passed here\n",
    "    # The API key is already present here, but it also needs to be passed to the OpenAI LLM\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key='YourKEY')\n",
    "    vectorstore = FAISS.from_texts(texts, embedding=embeddings)\n",
    "\n",
    "    # Create QA chain\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    # Pass the openai_api_key to the OpenAI LLM as well\n",
    "    qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0, openai_api_key='YOURKEY'), retriever=retriever)\n",
    "    return qa\n",
    "\n",
    "# Create the RAG agent\n",
    "qa = process_and_create_rag_from_df(df)\n",
    "\n",
    "# UI for questions\n",
    "query_box = widgets.Text(\n",
    "    placeholder='Ask a question based on your CSV...',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_query_submit(change):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        response = qa.run(query_box.value)\n",
    "        print(\"Answer:\", response)\n",
    "\n",
    "query_box.on_submit(on_query_submit)\n",
    "\n",
    "display(query_box, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "You successfully deployed and tested the AI Service! You can now view\n",
    "your deployment and test it as a REST API endpoint.\n",
    "\n",
    "<a id=\"copyrights\"></a>\n",
    "### Copyrights\n",
    "\n",
    "Licensed Materials - Copyright Â© 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\n",
    "Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "\n",
    "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
    "\n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
